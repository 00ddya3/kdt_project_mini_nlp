{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nessessary Dependancy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- python 3.9\n",
    "- pip upgrade // python -m pip install --upgrade pip\n",
    "- transformers\n",
    "- sentencepiece\n",
    "- tensorflow_addons\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> << reference >> </h4>\n",
    "\n",
    "- for fixing fitting Error\n",
    "\n",
    "https://github.com/hwangtaemin/NLP-with-Naver-movie-review/blob/main/movie_review.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install packages\n",
    "\n",
    "# ! python -m pip install --upgrade pip\n",
    "# ! pip install transformers\n",
    "# ! pip install sentencepiece\n",
    "# ! pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Imports</b><h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import * # For unstable module import error\n",
    "import tensorflow_addons as tfa # for optimizer\n",
    "\n",
    "from source.const import RANDOM_STATE,SEQUENCE_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Loading Data</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train :  (1500, 3)\n",
      "shape of test :  (500, 3)\n"
     ]
    }
   ],
   "source": [
    "# for developing\n",
    "train = pd.read_csv(\"./data/ratings_train.txt\",sep='\\t').sample(frac=0.01,random_state=RANDOM_STATE)\n",
    "test = pd.read_csv(\"./data/ratings_test.txt\",sep='\\t').sample(frac=0.01,random_state=RANDOM_STATE)\n",
    "\n",
    "print('shape of train : ',train.shape)\n",
    "print('shape of test : ',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Preprocessing</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at C:\\Users\\kdh/.cache\\huggingface\\transformers\\eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\kdh/.cache\\huggingface\\transformers\\f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\kdh/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get BERT pre-trained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na\n",
    "train.dropna(axis=0,how='any',inplace=True)\n",
    "test.dropna(axis=0,how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,'label'] = train['label'].astype(int)\n",
    "test.loc[:,'label'] = test['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface\n",
    "\n",
    "# convert data into bert input\n",
    "def get_bert_input(df): # need label\n",
    "    global tokenizer # do not miss it\n",
    "    \n",
    "    ts,segs,msks,lbs = [],[],[],[]\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        tokenized = tokenizer.encode(df['document'].iloc[i], max_length=SEQUENCE_LEN, truncation=True, padding='max_length')\n",
    "        zeros =  tokenized.count(0)\n",
    "        \n",
    "        segment = [0]*SEQUENCE_LEN\n",
    "        mask = [1]*(SEQUENCE_LEN-zeros) + [0]*zeros # Non-zero with zero mask\n",
    "        label = df['label'].iloc[i]\n",
    "        \n",
    "        ts.append(tokenized)\n",
    "        segs.append(segment)\n",
    "        msks.append(mask)\n",
    "        lbs.append(label)\n",
    "    \n",
    "    ts = np.array(ts)\n",
    "    segs = np.array(segs)\n",
    "    msks = np.array(msks)\n",
    "    lbs = np.array(lbs)\n",
    "    \n",
    "    return [ts,segs,msks],lbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Data preparation</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:00<00:00, 5000.01it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5262.89it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = get_bert_input(train)\n",
    "X_test,y_test = get_bert_input(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>BERT Modeling</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from\n",
    "https://huggingface.co/ , https://huggingface.co/bert-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "opt = tfa.optimizers.RectifiedAdam(learning_rate=1.0e-5, weight_decay=0.0025, warmup_proportion=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\kdh/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tf_model.h5 from cache at C:\\Users\\kdh/.cache\\huggingface\\transformers\\879ba3c37de5c396bee982a9383f64cf08c9cc966d66742254a3904e6719357f.53d9b251a2a9d5d86139b64555b5e8deb0a20fc53f8a5ee958ddbb4506125a0b.h5\n",
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " word (InputLayer)              [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " mask (InputLayer)              [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " segment (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  177853440   ['word[0][0]',                   \n",
      "                                thPoolingAndCrossAt               'mask[0][0]',                   \n",
      "                                tentions(last_hidde               'segment[0][0]']                \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            769         ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 177,854,209\n",
      "Trainable params: 177,854,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# modeling\n",
    "# we gonna use pre-trained model\n",
    "# bert base multilingual cased model\n",
    "model = TFBertModel.from_pretrained('bert-base-multilingual-cased') \n",
    "\n",
    "# indices must be integer\n",
    "# input layers\n",
    "token_inputs = tf.keras.layers.Input((SEQUENCE_LEN,),dtype=tf.dtypes.int32, name='word')\n",
    "mask_inputs = tf.keras.layers.Input((SEQUENCE_LEN,),dtype=tf.dtypes.int32, name='mask')\n",
    "segment_inputs = tf.keras.layers.Input((SEQUENCE_LEN,),dtype=tf.dtypes.int32, name='segment')\n",
    "\n",
    "bert_outputs = model([token_inputs,mask_inputs,segment_inputs])\n",
    "\n",
    "bert_outputs = bert_outputs[1]\n",
    "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)\n",
    "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
    "\n",
    "sentiment_model.compile(optimizer=opt, loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>FAIELD</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/tf_bert_model/bert/encoder/layer_._1/attention/output/dropout_5/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2900, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3301, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\kdh\\AppData\\Local\\Temp\\ipykernel_5416\\3203460272.py\", line 2, in <cell line: 2>\n      sentiment_model.fit(X_train, y_train,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1130, in call\n      outputs = self.bert(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 871, in call\n      encoder_outputs = self.encoder(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 535, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 541, in call\n      layer_outputs = layer_module(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 451, in call\n      self_attention_outputs = self.attention(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 377, in call\n      attention_output = self.dense_output(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 340, in call\n      hidden_states = self.dropout(inputs=hidden_states, training=training)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 111, in call\n      output = control_flow_util.smart_cond(training, dropped_inputs,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 105, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 108, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\backend.py\", line 1940, in dropout\n      return tf.nn.dropout(inputs, rate=rate, noise_shape=noise_shape,\nNode: 'model/tf_bert_model/bert/encoder/layer_._1/attention/output/dropout_5/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[100,128,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/tf_bert_model/bert/encoder/layer_._1/attention/output/dropout_5/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_28815]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kdh\\Desktop\\projects\\project_mini_nlp\\sentiment_classification_transformer.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kdh/Desktop/projects/project_mini_nlp/sentiment_classification_transformer.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kdh/Desktop/projects/project_mini_nlp/sentiment_classification_transformer.ipynb#ch0000017?line=1'>2</a>\u001b[0m sentiment_model\u001b[39m.\u001b[39;49mfit(X_train, y_train, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kdh/Desktop/projects/project_mini_nlp/sentiment_classification_transformer.ipynb#ch0000017?line=2'>3</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kdh/Desktop/projects/project_mini_nlp/sentiment_classification_transformer.ipynb#ch0000017?line=4'>5</a>\u001b[0m \u001b[39m# save weight\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kdh/Desktop/projects/project_mini_nlp/sentiment_classification_transformer.ipynb#ch0000017?line=5'>6</a>\u001b[0m sentiment_model\u001b[39m.\u001b[39msave_weights(\u001b[39m'\u001b[39m\u001b[39m./model/huggingface_BERT.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Users/kdh/miniconda3/envs/test/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/tf_bert_model/bert/encoder/layer_._1/attention/output/dropout_5/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2900, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3301, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\kdh\\AppData\\Local\\Temp\\ipykernel_5416\\3203460272.py\", line 2, in <cell line: 2>\n      sentiment_model.fit(X_train, y_train,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1130, in call\n      outputs = self.bert(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 871, in call\n      encoder_outputs = self.encoder(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 535, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 541, in call\n      layer_outputs = layer_module(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 451, in call\n      self_attention_outputs = self.attention(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 377, in call\n      attention_output = self.dense_output(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 340, in call\n      hidden_states = self.dropout(inputs=hidden_states, training=training)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 111, in call\n      output = control_flow_util.smart_cond(training, dropped_inputs,\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 105, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 108, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\kdh\\miniconda3\\envs\\test\\lib\\site-packages\\keras\\backend.py\", line 1940, in dropout\n      return tf.nn.dropout(inputs, rate=rate, noise_shape=noise_shape,\nNode: 'model/tf_bert_model/bert/encoder/layer_._1/attention/output/dropout_5/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[100,128,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/tf_bert_model/bert/encoder/layer_._1/attention/output/dropout_5/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_28815]"
     ]
    }
   ],
   "source": [
    "# training\n",
    "sentiment_model.fit(X_train, y_train, \n",
    "                    epochs=4, shuffle=True, batch_size=100, validation_data=(X_test, y_test))\n",
    "\n",
    "# save weight\n",
    "sentiment_model.save_weights('./model/huggingface_BERT.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Evaluation</b></h2></center>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddc820cf34ad67fe8490fc2f2da97ffc2731598be58de02108267b0519cec5ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
