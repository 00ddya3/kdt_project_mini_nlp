{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Imports</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Loading Data</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>판타지를 넘어 영화 역사에 남을 명작이다 내가 이걸 왜 극장에서 못봤을까 폰으로 봤...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이집트여행하는느낌의 영화</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>목포 연설 장면은 넋을 잃고 보게 된다</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  score  sentiment\n",
       "0  판타지를 넘어 영화 역사에 남을 명작이다 내가 이걸 왜 극장에서 못봤을까 폰으로 봤...   10.0        1.0\n",
       "1                                      이집트여행하는느낌의 영화    8.0        1.0\n",
       "2                              목포 연설 장면은 넋을 잃고 보게 된다   10.0        1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/processed/df.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>가까스로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>가령</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>각</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>각각</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word\n",
       "0     가\n",
       "1  가까스로\n",
       "2    가령\n",
       "3     각\n",
       "4    각각"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words\n",
    "\n",
    "stopwords = pd.read_table('./data/stopwords.txt')\n",
    "# ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Data Preprocessing</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7465/7465 [00:14<00:00, 503.64it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for sentence in tqdm(df['review'][:7465]): # 데이터의 80%를 train 데이터로 쪼갬\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['판타지', '를', '넘다', '영화', '역사', '에', '남', '을', '명작', '이다', '내', '가', '이', '걸', '왜', '극장', '에서', '못', '보다', '폰', '으로', '보다', '때', '의', '감동', '과는', '비교', '가', '안되다'], ['이집트', '여행', '하', '는', '느낌', '의', '영화'], ['목포', '연설', '장면', '은', '넋', '을', '잃다', '보다', '되다']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:03<00:00, 546.59it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for sentence in tqdm(df['review'][7465:]): # 데이터의 20%를 test 데이터로 쪼갬\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Intger Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train) # 단어 집합이 생성되는 동시에 고유한 정수가 부여됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '하다', '영화', '보다', '의']\n"
     ]
    }
   ],
   "source": [
    "print(list(tokenizer.word_index)[:5]) # 단어가 11,000 개가 넘게 존재하며, 부여된 고유한 정수를 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 정수는 전체 훈련 데이터에서 등장 빈도수가 높은 순서대로 부여되었기 때문에, 높은 정수가 부여된 단어들은 등장 빈도수가 매우 낮다는 것을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 빈도수가 낮은 단어들은 자연어 처리에서 배제하고자 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장 빈도수가 3회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해볼 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 11410\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 7073\n",
      "단어 집합에서 희귀 단어의 비율: 61.98948290972831\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 6.065065251784148\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 4338\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이를 케라스 토크나이저의 인자로 넘겨주고 텍스트 시퀀스를 정수 시퀀스로 변환함\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[604, 10, 165, 3, 624, 6, 330, 7, 170, 16, 63, 8, 1, 133, 72, 397, 32, 57, 4, 22, 4, 69, 5, 43, 665, 737, 8, 132], [2885, 1247, 158, 13, 64, 5, 3], [2529, 89, 11, 7, 894, 4, 21]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 53, 40, 25, 96, 3044, 218, 3, 2266, 96, 509, 3, 9, 25, 4, 148, 124, 40, 175, 49, 3631, 48, 4], [7, 1157, 204, 53, 21], [68, 30, 197, 55, 3132, 64, 31, 154, 3215, 174, 1030, 154, 8, 48, 235]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7465 1866\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(df['sentiment'][:7465])\n",
    "y_test = np.array(df['sentiment'][7465:])\n",
    "\n",
    "print(len(y_train),len(y_test)) # 길이를 보아하니 잘 나누어 진 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove empty samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sentence which length is less than 1\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(drop_train)) # 길이가 0인 샘플은 10개가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7465\n",
      "7465\n",
      "7455\n",
      "7455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdh\\miniconda3\\envs\\nlp\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 339\n",
      "리뷰의 평균 길이 : 18.23849765258216\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "\n",
    "# check sentence length distribution\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 긴 리뷰의 길이는 339이며, 그래프를 봤을 때 전체 데이터의 길이 분포는 대체적으로 약 25내외의 길이를 가지는 것을 볼 수 있슴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 처리할 수 있도록 X_train과 X_test의 모든 샘플의 길이를 특정 길이로 동일하게 맞춰줄 필요가 있슴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 길이 변수를 max_len으로 정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 리뷰가 내용이 잘리지 않도록 할 수 있는 최적의 max_len의 값은 얼마인지 확인하기 위해, 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    count = 0\n",
    "    for sentence in nested_list:\n",
    "        if(len(sentence) <= max_len):\n",
    "            count = count + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 94.19181757209925\n"
     ]
    }
   ],
   "source": [
    "max_len = 50\n",
    "\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 훈련 데이터 중 약 94%의 리뷰가 50이하의 길이를 가지는 것을 확인함 모든 샘플의 길이를 50으로 맞추어 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>LSTM Modeling</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "94/94 [==============================] - 3s 18ms/step - loss: 0.5997 - acc: 0.6652 - val_loss: 0.5241 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.73441, saving model to ./model\\lnh_lstm_model.h5\n",
      "Epoch 2/15\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.4264 - acc: 0.8115 - val_loss: 0.4670 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.73441 to 0.77197, saving model to ./model\\lnh_lstm_model.h5\n",
      "Epoch 3/15\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.3552 - acc: 0.8421 - val_loss: 0.4770 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.77197\n",
      "Epoch 4/15\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.3136 - acc: 0.8677 - val_loss: 0.4842 - val_acc: 0.7854\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77197 to 0.78538, saving model to ./model\\lnh_lstm_model.h5\n",
      "Epoch 5/15\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 0.2861 - acc: 0.8825 - val_loss: 0.5779 - val_acc: 0.7451\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.78538\n",
      "Epoch 6/15\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.2629 - acc: 0.8949 - val_loss: 0.5495 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78538\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('./model/lnh_lstm_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 4ms/step - loss: 0.5907 - acc: 0.7583\n",
      "\n",
      " 테스트 정확도: 0.7583\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>Evaluation</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    loaded_model = load_model('./model/lnh_lstm_model.h5')\n",
    "    \n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    \n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    \n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    \n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.73% 확률로 긍정 리뷰입니다.\n",
      "\n",
      "97.07% 확률로 부정 리뷰입니다.\n",
      "\n",
      "96.69% 확률로 부정 리뷰입니다.\n",
      "\n",
      "77.51% 확률로 부정 리뷰입니다.\n",
      "\n",
      "66.25% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')\n",
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')\n",
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')\n",
    "sentiment_predict('감독 뭐하는 놈이냐?')\n",
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4fe7a65e16781ebce1fdb8c74096656eac6964dbb1774f4c65d543e2bcc2ead"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
