{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 데이터셋 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/processed/df.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 불용어 import 및 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_table('./data/stopwords.txt')\n",
    "#['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) OTK를 사용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7465/7465 [00:33<00:00, 222.27it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for sentence in tqdm(df['review'][:7465]): # 데이터의 80%를 train 데이터로 쪼갬\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['review'][:7466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['판타지', '를', '넘다', '영화', '역사', '에', '남', '을', '명작', '이다', '내', '가', '이', '걸', '왜', '극장', '에서', '못', '보다', '폰', '으로', '보다', '때', '의', '감동', '과는', '비교', '가', '안되다'], ['이집트', '여행', '하', '는', '느낌', '의', '영화'], ['목포', '연설', '장면', '은', '넋', '을', '잃다', '보다', '되다']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:11<00:00, 168.19it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for sentence in tqdm(df['review'][7465:]): # 데이터의 20%를 test 데이터로 쪼갬\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train) # 단어 집합이 생성되는 동시에 고유한 정수가 부여됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.word_index) # 단어가 11,000 개가 넘게 존재하며, 부여된 고유한 정수를 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 정수는 전체 훈련 데이터에서 등장 빈도수가 높은 순서대로 부여되었기 때문에, 높은 정수가 부여된 단어들은 등장 빈도수가 매우 낮다는 것을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 빈도수가 낮은 단어들은 자연어 처리에서 배제하고자 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장 빈도수가 3회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해볼 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 11410\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 7073\n",
      "단어 집합에서 희귀 단어의 비율: 61.98948290972831\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 6.065065251784148\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈도수 2이하인 단어를 제거한 후의 단어집합의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 4338\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 집합의 크기는 4,338개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 케라스 토크나이저의 인자로 넘겨주고 텍스트 시퀀스를 정수 시퀀스로 변환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[604, 10, 165, 3, 624, 6, 330, 7, 170, 16, 63, 8, 1, 133, 72, 397, 32, 57, 4, 22, 4, 69, 5, 43, 665, 737, 8, 132], [2885, 1247, 158, 13, 64, 5, 3], [2529, 89, 11, 7, 894, 4, 21]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3]) # X_train 정수인코딩 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 53, 40, 25, 96, 3044, 218, 3, 2266, 96, 509, 3, 9, 25, 4, 148, 124, 40, 175, 49, 3631, 48, 4], [7, 1157, 204, 53, 21], [68, 30, 197, 55, 3132, 64, 31, 154, 3215, 174, 1030, 154, 8, 48, 235]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[:3]) # X_test 정수인코딩 확인용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 샘플 내의 단어들은 각 단어에 대한 정수로 변환된 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 개수는 4,338개로 제한되었으므로 \"0번 단어 ~ 4,338번 단어\" 까지만 사용 중임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0번 단어는 패딩을 위한 토큰임을 상기하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data에서 y_train과 y_test를 별도로 저장해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(df['sentiment'][:7465])\n",
    "y_test = np.array(df['sentiment'][7465:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7465 1866\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train),len(y_test)) # 길이를 보아하니 잘 나누어 진 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 빈 샘플(empty samples) 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 빈(empty) 샘플이 되었다는 것을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈 샘플들은 어떤 레이블이 붙어있던 의미가 없으므로 빈 샘플들을 제거해주는 작업을 하겠슴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 샘플들의 길이를 확인해서 길이가 0인 샘플들의 인덱스를 받아오겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1] # X_train에서 길이가 0인 샘플 index리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(drop_train)) # 길이가 0인 샘플은 10개가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7455\n",
      "7455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\Linux\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7455 개로 샘플의 수가 10개 줄어든 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠슴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 339\n",
      "리뷰의 평균 길이 : 18.23849765258216\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 긴 리뷰의 길이는 339이며, 그래프를 봤을 때 전체 데이터의 길이 분포는 대체적으로 약 25내외의 길이를 가지는 것을 볼 수 있슴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 처리할 수 있도록 X_train과 X_test의 모든 샘플의 길이를 특정 길이로 동일하게 맞춰줄 필요가 있슴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 길이 변수를 max_len으로 정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 리뷰가 내용이 잘리지 않도록 할 수 있는 최적의 max_len의 값은 얼마인지 확인하기 위해, 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    count = 0\n",
    "    for sentence in nested_list:\n",
    "        if(len(sentence) <= max_len):\n",
    "            count = count + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 분포 그래프를 봤을 때, max_len = 50이 적당할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 값이 얼마나 많은 리뷰 길이를 커버하는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 94.19181757209925\n"
     ]
    }
   ],
   "source": [
    "max_len = 50\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 훈련 데이터 중 약 94%의 리뷰가 50이하의 길이를 가지는 것을 확인함 모든 샘플의 길이를 50으로 맞추어 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) LSTM으로 네이버 영화 리뷰 감성 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "94/94 [==============================] - 9s 77ms/step - loss: 0.5943 - acc: 0.6683 - val_loss: 0.4949 - val_acc: 0.7425\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.74245, saving model to lnh_lstm_model.h5\n",
      "Epoch 2/15\n",
      "94/94 [==============================] - 7s 73ms/step - loss: 0.4244 - acc: 0.8099 - val_loss: 0.4981 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.74245 to 0.74648, saving model to lnh_lstm_model.h5\n",
      "Epoch 3/15\n",
      "94/94 [==============================] - 7s 73ms/step - loss: 0.3652 - acc: 0.8422 - val_loss: 0.4673 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74648 to 0.77062, saving model to lnh_lstm_model.h5\n",
      "Epoch 4/15\n",
      "94/94 [==============================] - 7s 73ms/step - loss: 0.3142 - acc: 0.8670 - val_loss: 0.4851 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77062 to 0.77129, saving model to lnh_lstm_model.h5\n",
      "Epoch 5/15\n",
      "94/94 [==============================] - 7s 72ms/step - loss: 0.2898 - acc: 0.8803 - val_loss: 0.5117 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77129 to 0.77264, saving model to lnh_lstm_model.h5\n",
      "Epoch 6/15\n",
      "94/94 [==============================] - 7s 73ms/step - loss: 0.2613 - acc: 0.8939 - val_loss: 0.5684 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77264\n",
      "Epoch 7/15\n",
      "94/94 [==============================] - 7s 72ms/step - loss: 0.2355 - acc: 0.9051 - val_loss: 0.5707 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.77264 to 0.77465, saving model to lnh_lstm_model.h5\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('./lnh_lstm_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 1s 15ms/step - loss: 0.6124 - acc: 0.7567\n",
      "\n",
      " 테스트 정확도: 0.7567\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('lnh_lstm_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) 리뷰 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.28% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.60% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.81% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.75% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('감독 뭐하는 놈이냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.08% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4fe7a65e16781ebce1fdb8c74096656eac6964dbb1774f4c65d543e2bcc2ead"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
