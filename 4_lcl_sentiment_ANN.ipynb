{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCnNo3EWm18E"
      },
      "source": [
        "### **Install dependancy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8uRA1lYNsi-l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in c:\\users\\kdh\\miniconda3\\envs\\nlp\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\kdh\\miniconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in c:\\users\\kdh\\miniconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\kdh\\miniconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (4.8.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\kdh\\miniconda3\\envs\\nlp\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><h2><b>Imports</b></h2></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VKnJpLXasZ0M"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "from tensorflow.keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9QlU-PGnBi6"
      },
      "source": [
        "<center><h2><b>Loading Data</b></h2></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zwkocwWJsbf2"
      },
      "outputs": [],
      "source": [
        "def open_txt(filename):\n",
        "    with open(filename, 'r',encoding = 'utf-8') as f:  \n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        data = data[1:]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mnpDa3QAsrsc"
      },
      "outputs": [],
      "source": [
        "train_data = open_txt('./data/train_sample.txt')\n",
        "test_data = open_txt('./data/test_sample.txt')\n",
        "\n",
        "df = pd.read_csv('./data/df_notk.csv', encoding = 'cp949')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FPwHVB9pZ4m"
      },
      "source": [
        "<center><h2><b>Preprocessing</b></h2></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "okt = Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 하나의 문장을 토큰화 한 후 텍스트와 품사태깅을 / 구분자로 묶어준다.\n",
        "def tokenizing(docs):\n",
        "    return ['/'.join(t) for t in okt.pos(docs, norm=True, stem=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CdLISLd7tZyg"
      },
      "outputs": [],
      "source": [
        "# parse to list\n",
        "train_list, test_list = [], []  \n",
        "\n",
        "for i in train_data:\n",
        "    try:\n",
        "        train_value = [tokenizing(i[1]), i[2]]\n",
        "        train_list.append(train_value)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "for i in test_data:\n",
        "    try:\n",
        "        test_value = [tokenizing(i[1]), i[2]]\n",
        "        test_list.append(test_value)\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EpQGTJlLtfyS"
      },
      "outputs": [],
      "source": [
        "tokens = [t for d in train_list for t in d[0]]\n",
        "\n",
        "text = nltk.Text(tokens,name='NMSC')\n",
        "text.vocab().most_common(10) #vocab().most_common(10) - 텍스트 빈도 상위 10개 보여주기\n",
        "\n",
        "selected_words = [f[0] for f in text.vocab().most_common(10000)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def term_frequency(doc):\n",
        "    return [doc.count(word) for word in selected_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8UFbEuxHtjWi"
      },
      "outputs": [],
      "source": [
        "# 1번째리뷰를 상위 10000개와 각각 매칭하여 각 10000개의 단어가 해당 문장에 얼마나 포함되는지를 확인\n",
        "train_x = [term_frequency(d) for d, _ in train_list]\n",
        "test_x = [term_frequency(d) for d, _ in test_list]\n",
        "\n",
        "train_y = [c for _, c in train_list]\n",
        "test_y = [c for _, c in test_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><h2><b>Data Preparation</b></h2></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iHYJhRFvtuUA"
      },
      "outputs": [],
      "source": [
        "x_train = np.asarray(train_x).astype('float32')\n",
        "x_test = np.asarray(test_x).astype('float32')\n",
        "\n",
        "y_train = np.asarray(train_y).astype('float32')\n",
        "y_test = np.asarray(test_y).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZe3pSEjoVLs"
      },
      "source": [
        "<center><h2><b>Modeling</b></h2></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0t_dxOPtw2x",
        "outputId": "7e850de0-d46a-4e5d-c93b-598d34f4f5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                640064    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 644,289\n",
            "Trainable params: 644,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "79/79 [==============================] - 3s 9ms/step - loss: 0.4712 - binary_accuracy: 0.7978\n",
            "Epoch 2/10\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.3397 - binary_accuracy: 0.8561\n",
            "Epoch 3/10\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.2968 - binary_accuracy: 0.8760\n",
            "Epoch 4/10\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.2629 - binary_accuracy: 0.8906\n",
            "Epoch 5/10\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.2307 - binary_accuracy: 0.9050\n",
            "Epoch 6/10\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.2011 - binary_accuracy: 0.9205\n",
            "Epoch 7/10\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.1735 - binary_accuracy: 0.9327\n",
            "Epoch 8/10\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.1501 - binary_accuracy: 0.9453\n",
            "Epoch 9/10\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.1266 - binary_accuracy: 0.9543\n",
            "Epoch 10/10\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.1068 - binary_accuracy: 0.9624\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1b7645fe9e8>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ANN\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
        "              loss=losses.binary_crossentropy,\n",
        "              metrics=[metrics.binary_accuracy])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# train\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=512)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><h2><b>Evaluation</b></h2></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6513 - binary_accuracy: 0.8264\n",
            "Loss : 0.6513171195983887, Accuracy : 0.8263999819755554\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Loss : {}, Accuracy : {}'.format(*results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCBFg2oQ1Q_y",
        "outputId": "ecf28ca0-98e4-4b87-aadb-f6314770617f"
      },
      "outputs": [],
      "source": [
        "# predict samples\n",
        "def input_text(text):\n",
        "    token = tokenizing(text)\n",
        "    tf =term_frequency(token)\n",
        "\n",
        "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
        "\n",
        "    score = float(model.predict(data)) #새로운 데이터를 받으면 결과 예측\n",
        "    if(score > 0.5):\n",
        "        print(\"{} : 긍정 [{:.2f}%] \\n\".format(text, score * 100))\n",
        "    else:\n",
        "        print(\"{} : 부정 [{:.2f}%] \\n\".format(text, (1 - score) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "판타지를 넘어 영화 역사에 남을 명작이다. 내가 이걸 왜 극장에서 못봤을까.... 폰으로 봤을때의 감동과는 비교가 안될텐데.... : 부정 [56.61%] \n",
            "\n",
            "이집트여행하는느낌의 영화 : 긍정 [98.09%] \n",
            "\n",
            "목포 연설 장면은 넋을 잃고 보게 된다. : 긍정 [98.38%] \n",
            "\n",
            "그저 그렇네요. 뻔한 반전과 결말. : 부정 [99.17%] \n",
            "\n",
            "더 배트맨 조커처럼 몰입감이 있으면 추천 반대로 지루하면 비추천 : 부정 [92.08%] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(5) :\n",
        "    input_text(df['review'][i])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "keras감정분석.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
